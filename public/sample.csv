A company makes forecasts each quarter to decide how to optimize operations to meet expected demand. The company uses ML models to make these forecasts. An AI practitioner is writing a report about the trained ML models to provide transparency and explainability to company stakeholders.What should the AI practitioner include in the report to meet the transparency and explainability requirements?,Partial dependence plots (PDPs),Code for model training;Sample data for training;Model convergence tables," Partial dependence plots (PDPs) provide transparency and explainability by showing how individual features affect model predictions, helping stakeholders understand the model's behavior. ", A: Code for model training is not easily interpretable by non-technical stakeholders and doesn't directly explain the model's decision-making process. C: Sample data alone doesn't provide insight into how the model uses this data to make decisions. D: Model convergence tables show training progress but don't explain how the model uses features to make predictions. 
A law firm wants to build an AI application by using large language models (LLMs). The application will read legal documents and extract key points from the documents. Which solution meets these requirements?,Develop a summarization chatbot,Build an automatic named entity recognition system; Create a recommendation engine; Develop a multi-language translation system," Developing a summarization chatbot is the most suitable solution for extracting key points from legal documents using LLMs, as it can process and condense large amounts of text into essential information. "," A: Named entity recognition identifies specific entities but doesn't summarize or extract key points from entire documents. B: A recommendation engine suggests items based on preferences, which is unrelated to extracting key points from legal documents. D: Multi-language translation is unnecessary if the documents are in a single language and doesn't address the task of extracting key points. "
A company wants to classify human genes into 20 categories based on gene characteristics. The company needs an ML algorithm to document how the inner mechanism of the model affects the output. Which ML algorithm meets these requirements?,Decision trees,Linear regression; Logistic regression; Neural networks," Decision trees provide a clear, interpretable structure that shows how decisions are made based on input features, making it easy to document the inner mechanism of the model. "," B: Linear regression may not capture complex relationships in gene characteristics and doesn't provide a clear decision path. C: Logistic regression doesn't naturally extend to 20 categories and lacks the hierarchical decision structure of trees. D: Neural networks are often considered ""black box"" models, making them difficult to interpret and document for stakeholders. "
A company has built an image classification model to predict plant diseases from photos of plant leaves. The company wants to evaluate how many images the model classified correctly. Which evaluation metric should the company use to measure the model's performance?,Accuracy,R-squared score; Root mean squared error (RMSE); Learning rate," Accuracy is the most appropriate metric for evaluating image classification models, as it measures the proportion of correctly classified images across all classes. "," A: R-squared is used for regression tasks, not for classification. C: RMSE is a regression metric that measures the average magnitude of prediction errors, unsuitable for classification tasks. D: Learning rate is a hyperparameter used during model training, not an evaluation metric for model performance. "
A company is using a pre-trained large language model (LLM) to build a chatbot for product recommendations. The company needs the LLM outputs to be short and written in a specific language. Which solution will align the LLM response quality with the company's expectations?,Adjust the prompt,Choose an LLM of a different size; Increase the temperature; Increase the Top K value," Adjusting the prompt is the most effective way to control the output of an LLM, including specifying the desired language and length of responses. "," B: Changing model size doesn't guarantee specific output characteristics like language or length. C: Increasing temperature leads to more random and diverse outputs, potentially making responses less controlled. D: Top K affects the diversity of token selection but doesn't directly control language or response length. "
A company uses Amazon SageMaker for its ML pipeline in a production environment. The company has large input data sizes up to 1 GB and processing times up to 1 hour. The company needs near real-time latency. Which SageMaker inference option meets these requirements?,Asynchronous inference,Serverless inference; Real-time inference; Batch transform,"It supports large input data sizes up to 1 GB, which matches the company's need for ""large input data sizes up to 1 GB"".
It allows for long processing times up to 15 minutes, which covers the company's requirement of ""processing times up to 1 hour"".
It is designed for workloads that do not have sub-second latency requirements, which aligns with the company's need for ""near real-time"" processing rather than strict real-time requirements","A. Real-time inference: While this option provides low latency, it only supports payload sizes up to 6 MB and processing times up to 60 seconds, which falls short of the company's requirements for large input sizes and longer processing times.
B. Serverless inference: This option is best for unpredictable or intermittent traffic patterns, which doesn't match the company's specific needs. It also has limitations on payload size (up to 4 MB) and processing time (up to 60 seconds) that don't meet the stated requirements.
D. Batch transform: This option is designed for offline processing of large datasets and doesn't provide the near real-time capability required by the company."
"A company is using domain-specific models. The company wants to avoid creating new models from the beginning. The company instead wants to adapt pre-trained models to create models for new, related tasks. Which ML strategy meets these requirements?",Use transfer learning,Increase the number of epochs; Decrease the number of epochs; Use unsupervised learning," Transfer learning allows adapting pre-trained models to new, related tasks without starting from scratch, which aligns with the company's requirements. "," A and C: Changing the number of epochs doesn't adapt models to new tasks, it only affects training duration. D: Unsupervised learning doesn't leverage pre-trained models for new tasks. "
A company is building a solution to generate images for protective eyewear. The solution must have high accuracy and must minimize the risk of incorrect annotations. Which solution will meet these requirements?,Human-in-the-loop validation by using Amazon SageMaker Ground Truth Plus,Data augmentation by using an Amazon Bedrock knowledge base; Image recognition by using Amazon Rekognition; Data summarization by using Amazon QuickSight Q, Human-in-the-loop validation using Amazon SageMaker Ground Truth Plus ensures high accuracy and minimizes the risk of incorrect annotations by incorporating human oversight. , B: Data augmentation doesn't directly improve annotation accuracy for protective eyewear images. C: Image recognition alone doesn't ensure accurate annotations for specific use cases. D: Data summarization isn't relevant for image annotation tasks. 
A company wants to create a chatbot by using a foundation model (FM) on Amazon Bedrock. The FM needs to access encrypted data that is stored in an Amazon S3 bucket. The data is encrypted with Amazon S3 managed keys (SSE-S3). The FM encounters a failure when attempting to access the S3 bucket data. Which solution will meet these requirements?,Ensure that the role that Amazon Bedrock assumes has permission to decrypt data with the correct encryption key,Set the access permissions for the S3 buckets to allow public access to enable access over the internet; Use prompt engineering techniques to tell the model to look for information in Amazon S3; Ensure that the S3 data does not contain sensitive information, Ensuring that the role Amazon Bedrock assumes has permission to decrypt data with the correct encryption key is crucial for the FM to access encrypted data in the S3 bucket. , B: Setting public access is a security risk and doesn't solve the permission issue. C: Prompt engineering doesn't solve permission issues for accessing encrypted data. D: The presence of sensitive information isn't the problem; it's about access permissions. 
A company wants to use language models to create an application for inference on edge devices. The inference must have the lowest latency possible. Which solution will meet these requirements?,Deploy optimized small language models (SLMs) on edge devices,Deploy optimized large language models (LLMs) on edge devices; Incorporate a centralized small language model (SLM) API for asynchronous communication with edge devices; Incorporate a centralized large language model (LLM) API for asynchronous communication with edge devices," Deploying optimized small language models (SLMs) on edge devices provides the lowest latency for inference, meeting the company's requirements. "," B: LLMs are too large for edge devices and would increase latency. C and D: Centralized APIs, whether SLM or LLM, introduce network latency, which is not optimal for edge inference. "
A company wants to build an ML model by using Amazon SageMaker. The company needs to share and manage variables for model development across multiple teams. Which SageMaker feature meets these requirements?,Amazon SageMaker Feature Store,Amazon SageMaker Data Wrangler; Amazon SageMaker Clarify; Amazon SageMaker Model Cards, Amazon SageMaker Feature Store is designed for sharing and managing variables (features) across multiple teams in model development. ," B: Data Wrangler is for data preparation, not feature management. C: Clarify is for bias detection and explainability, not feature management. D: Model Cards document model information but don't manage variables across teams. "
A company wants to use generative AI to increase developer productivity and software development. The company wants to use Amazon Q Developer. What can Amazon Q Developer do to help the company meet these requirements?,"Create software snippets, reference tracking, and open source license tracking",Run an application without provisioning or managing servers; Enable voice commands for coding and providing natural language search; Convert audio files to text documents by using ML models," Amazon Q Developer can create software snippets, track references, and monitor open source licenses, which aligns with increasing developer productivity and software development. "," B: This describes serverless computing, not Amazon Q Developer's capabilities. C: Voice commands for coding are not mentioned as a feature of Amazon Q Developer. D: Converting audio files to text is not a primary function of Amazon Q Developer. "
"A financial institution is using Amazon Bedrock to develop an AI application. The application is hosted in a VPC. To meet regulatory compliance standards, the VPC is not allowed access to any internet traffic. Which AWS service or feature will meet these requirements?",AWS PrivateLink,Amazon Macie; Amazon CloudFront; Internet gateway," AWS PrivateLink allows the company to access Amazon Bedrock from within their VPC without internet access, meeting regulatory compliance standards. "," B: Amazon Macie is for data security and privacy, not for network connectivity. C: Amazon CloudFront is a content delivery network, not suitable for this requirement. D: An internet gateway would violate the requirement of no internet access. "
"A company wants to develop an educational game where users answer questions such as the following: ""A jar contains six red, four green, and three yellow marbles. What is the probability of choosing a green marble from the jar?"" Which solution meets these requirements with the LEAST operational overhead?",Use code that will calculate probability by using simple rules and computations,Use supervised learning to create a regression model that will predict probability; Use reinforcement learning to train a model to return the probability; Use unsupervised learning to create a model that will estimate probability density, Using code that calculates probability using simple rules and computations is the most straightforward and least operationally complex solution for this type of educational game. , A: Supervised learning for regression is unnecessarily complex for simple probability calculations. B: Reinforcement learning is not appropriate for static probability calculations. D: Unsupervised learning to estimate probability density is overly complex for this simple scenario. 
Which metric measures the runtime efficiency of operating AI models?,Average response time,Customer satisfaction score (CSAT); Training time for each epoch; Number of training instances, Average response time measures the runtime efficiency of operating AI models by indicating how quickly the model can process and respond to inputs. ," A: CSAT measures user satisfaction, not runtime efficiency. B: Training time per epoch is related to model training, not operation. D: Number of training instances is about infrastructure, not runtime efficiency. "
A company is building a contact center application and wants to gain insights from customer conversations. The company wants to analyze and extract key information from the audio of the customer calls. Which solution meets these requirements?,Transcribe call recordings by using Amazon Transcribe,Build a conversational chatbot by using Amazon Lex; Extract information from call recordings by using Amazon SageMaker Model Monitor; Create classification labels by using Amazon Comprehend, Transcribing call recordings using Amazon Transcribe is the first step in analyzing and extracting key information from audio of customer calls. ," A: Building a chatbot doesn't directly analyze existing call recordings. C: SageMaker Model Monitor is for monitoring deployed models, not for audio analysis. D: Amazon Comprehend is for text analysis, not directly for audio processing. "
A company has petabytes of unlabeled customer data to use for an advertisement campaign. The company wants to classify its customers into tiers to advertise and promote the company’s products. Which methodology should the company use to meet these requirements?,Unsupervised learning,Supervised learning; Reinforcement learning; Reinforcement learning from human feedback (RLHF)," Unsupervised learning is ideal for classifying customers into tiers using unlabeled data, as it can discover patterns and groupings without predefined categories. "," A: Supervised learning requires labeled data, which is not available. C and D: Reinforcement learning is for decision-making processes, not for customer classification based on existing data. "
An AI practitioner wants to use a foundation model (FM) to design a search application. The search application must handle queries that have text and images. Which type of FM should the AI practitioner use to power the search application?,Multi-modal embedding model,Text embedding model; Multi-modal generation model; Image generation model," A multi-modal embedding model can handle queries with both text and images, making it suitable for a search application that needs to process both types of input. "," B: Text embedding model can't handle image inputs. C: Multi-modal generation model is for creating content, not for search. D: Image generation model is for creating images, not for search functionality. "
A company uses a foundation model (FM) from Amazon Bedrock for an AI search tool. The company wants to fine-tune the model to be more accurate by using the company's data. Which strategy will successfully fine-tune the model?,Provide labeled data with the prompt field and the completion field,Prepare the training dataset by creating a .txt file that contains multiple lines in .csv format; Purchase Provisioned Throughput for Amazon Bedrock; Train the model on journals and textbooks, Providing labeled data with prompt and completion fields is the correct approach for fine-tuning a foundation model to improve accuracy. , B: Preparing a .txt file in .csv format is not the correct data format for fine-tuning. C: Purchasing Provisioned Throughput doesn't fine-tune the model. D: Training on general journals and textbooks doesn't specifically fine-tune the model for company data. 
A company wants to use AI to protect its application from threats. The AI solution needs to check if an IP address is from a suspicious source. Which solution meets these requirements?,Develop an anomaly detection system,Build a speech recognition system; Create a natural language processing (NLP) named entity recognition system; Create a fraud forecasting system, Developing an anomaly detection system is the most appropriate solution for checking if an IP address is from a suspicious source. , A: Speech recognition is unrelated to IP address threat detection. B: NLP named entity recognition doesn't apply to IP address analysis. D: Fraud forecasting is too broad and not specific to IP threat detection. 
Which feature of Amazon OpenSearch Service gives companies the ability to build vector database applications?,Scalable index management and nearest neighbor search capability,Integration with Amazon S3 for object storage; Support for geospatial indexing and queries; Ability to perform real-time analysis on streaming data, Scalable index management and nearest neighbor search capability in Amazon OpenSearch Service enables building vector database applications. ," A: S3 integration is for storage, not vector search. B: Geospatial indexing is not specific to vector databases. D: Real-time analysis of streaming data is not directly related to vector database functionality. "
Which option is a use case for generative AI models?,Creating photorealistic images from text descriptions for digital marketing,Improving network security by using intrusion detection systems; Enhancing database performance by using optimized indexing; Analyzing financial data to forecast stock market trends, Creating photorealistic images from text descriptions for digital marketing is a prime use case for generative AI models. ," A: Improving network security is not a typical generative AI use case. C: Enhancing database performance is not related to generative AI. D: Analyzing financial data for forecasting is more suited to predictive, not generative, AI. "
A company wants to build a generative AI application by using Amazon Bedrock and needs to choose a foundation model (FM). The company wants to know how much information can fit into one prompt. Which consideration will inform the company’s decision?,Context window,Temperature; Batch size; Model size, The context window determines how much information can fit into one prompt for a foundation model. ," A: Temperature affects output randomness, not prompt capacity. C: Batch size is related to training, not prompt capacity. D: Model size affects overall capabilities but doesn't directly determine prompt capacity. "
A company wants to make a chatbot to help customers. The chatbot will help solve technical problems without human intervention. The company chose a foundation model (FM) for the chatbot. The chatbot needs to produce responses that adhere to company tone. Which solution meets these requirements?,Experiment and refine the prompt until the FM produces the desired responses,Set a low limit on the number of tokens the FM can produce; Use batch inferencing to process detailed responses; Define a higher number for the temperature parameter, Experimenting and refining the prompt until the FM produces the desired responses is the most effective way to ensure the chatbot adheres to company tone. , A: Setting a low token limit doesn't ensure adherence to company tone. B: Batch inferencing doesn't affect response tone. D: Increasing temperature would make responses less predictable and controlled. 
A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company wants to classify the sentiment of text passages as positive or negative. Which prompt engineering strategy meets these requirements?,Provide examples of text passages with corresponding positive or negative labels in the prompt followed by the new text passage to be classified,"Provide a detailed explanation of sentiment analysis and how LLMs work in the prompt; Provide the new text passage to be classified without any additional context or examples; Provide the new text passage with a few examples of unrelated tasks, such as text summarization or question answering"," Providing examples of text passages with corresponding positive or negative labels in the prompt, followed by the new text to be classified, is an effective few-shot learning approach for sentiment analysis. ", B: Providing a detailed explanation of sentiment analysis is unnecessary and inefficient. C: Providing the new text without context doesn't leverage the LLM's capabilities. D: Providing unrelated task examples would confuse the model. 
A security company is using Amazon Bedrock to run foundation models (FMs). The company wants to ensure that only authorized users invoke the models. The company needs to identify any unauthorized access attempts to set appropriate AWS Identity and Access Management (IAM) policies and roles for future iterations of the FMs. Which AWS service should the company use to identify unauthorized users that are trying to access Amazon Bedrock?,AWS CloudTrail,AWS Audit Manager; Amazon Fraud Detector; AWS Trusted Advisor, AWS CloudTrail can identify unauthorized users trying to access Amazon Bedrock by logging API calls and user activities. ," A: AWS Audit Manager is for assessing compliance, not real-time access monitoring. C: Amazon Fraud Detector is for detecting fraudulent online activities, not AWS service access. D: AWS Trusted Advisor provides best practice recommendations, not access monitoring. "
A company has developed an ML model for image classification. The company wants to deploy the model to production so that a web application can use the model. The company needs to implement a solution to host the model and serve predictions without managing any of the underlying infrastructure. Which solution will meet these requirements?,Use Amazon SageMaker Serverless Inference to deploy the model,Use Amazon CloudFront to deploy the model; Use Amazon API Gateway to host the model and serve predictions; Use AWS Batch to host the model and serve predictions, Amazon SageMaker Serverless Inference allows deployment of the model and serving predictions without managing underlying infrastructure. ," B: Amazon CloudFront is a content delivery network, not for model deployment. C: Amazon API Gateway is for creating APIs, not for hosting ML models. D: AWS Batch is for batch processing jobs, not for serving real-time predictions. "
An AI company periodically evaluates its systems and processes with the help of independent software vendors (ISVs). The company needs to receive email message notifications when an ISV's compliance reports become available. Which AWS service can the company use to meet this requirement?,AWS Artifact,AWS Audit Manager; AWS Trusted Advisor; AWS Data Exchange, AWS Artifact provides access to compliance reports and can send email notifications when new reports are available. ," A: AWS Audit Manager is for assessing compliance, not for accessing ISV reports. C: AWS Trusted Advisor provides best practice recommendations, not compliance reports. D: AWS Data Exchange is for sharing data, not specifically for compliance reports. "
A company wants to use a large language model (LLM) to develop a conversational agent. The company needs to prevent the LLM from being manipulated with common prompt engineering techniques to perform undesirable actions or expose sensitive information. Which action will reduce these risks?,Create a prompt template that teaches the LLM to detect attack patterns,Increase the temperature parameter on invocation requests to the LLM; Avoid using LLMs that are not listed in Amazon SageMaker; Decrease the number of input tokens on invocations of the LLM, Creating a prompt template that teaches the LLM to detect attack patterns can help prevent manipulation and protect sensitive information. ," B: Increasing temperature would make outputs less predictable, potentially increasing vulnerability. C: Avoiding non-SageMaker LLMs doesn't address the security concern. D: Decreasing input tokens could limit the model's ability to understand context and detect threats. "
A company is using the Generative AI Security Scoping Matrix to assess security responsibilities for its solutions. The company has identified four different solution scopes based on the matrix. Which solution scope gives the company the MOST ownership of security responsibilities?,Building and training a generative AI model from scratch by using specific data that a customer owns,Using a third-party enterprise application that has embedded generative AI features; Building an application by using an existing third-party generative AI foundation model (FM); Refining an existing third-party generative AI foundation model (FM) by fine-tuning the model by using data specific to the business, Building and training a generative AI model from scratch using specific customer-owned data gives the company the most ownership and control over security responsibilities. ," A, B, and C involve using third-party models or applications, which reduce the company's direct control and security responsibilities compared to building from scratch. "
An AI practitioner has a database of animal photos. The AI practitioner wants to automatically identify and categorize the animals in the photos without manual human effort. Which strategy meets these requirements?,Object detection,Anomaly detection; Named entity recognition; Inpainting, Object detection is the most appropriate strategy for automatically identifying and categorizing animals in photos without manual human effort. ," B: Anomaly detection is for identifying unusual patterns, not categorizing objects. C: Named entity recognition is for text, not images. D: Inpainting is for filling in missing parts of images, not object identification. "
A company wants to create an application by using Amazon Bedrock. The company has a limited budget and prefers flexibility without long-term commitment. Which Amazon Bedrock pricing model meets these requirements?,On-Demand,Model customization; Provisioned Throughput; Spot Instance," On-Demand pricing for Amazon Bedrock offers flexibility without long-term commitment, suitable for a company with a limited budget. "," B: Model customization is a feature, not a pricing model. C: Provisioned Throughput requires more commitment. D: Spot Instance is not a pricing option for Amazon Bedrock. "
Which AWS service or feature can help an AI development team quickly deploy and consume a foundation model (FM) within the team’s VPC?,Amazon SageMaker JumpStart,"Amazon Personalize; PartyRock, an Amazon Bedrock Playground; Amazon SageMaker endpoints", Amazon SageMaker JumpStart provides pre-trained models that can be quickly deployed within a team's VPC. ," A: Amazon Personalize is for building recommendation systems, not for deploying FMs. C: PartyRock is a playground, not for deploying within a VPC. D: SageMaker endpoints are for model deployment but don't specifically offer pre-trained FMs. "
How can companies use large language models (LLMs) securely on Amazon Bedrock?,Design clear and specific prompts. Configure AWS Identity and Access Management (IAM) roles and policies by using least privilege access.,Enable AWS Audit Manager for automatic model evaluation jobs; Enable Amazon Bedrock automatic model evaluation jobs; Use Amazon CloudWatch Logs to make models explainable and to monitor for bias," Designing clear, specific prompts and configuring IAM roles and policies with least privilege access are key practices for using LLMs securely on Amazon Bedrock. "," B and C: AWS Audit Manager and automatic model evaluation jobs don't directly address secure usage. D: CloudWatch Logs are for monitoring, not for making models explainable or monitoring bias. "
A company has terabytes of data in a database that the company can use for business analysis. The company wants to build an AI-based application that can build a SQL query from input text that employees provide. The employees have minimal experience with technology. Which solution meets these requirements?,Generative pre-trained transformers (GPT),Residual neural network; Support vector machine; WaveNet, Generative pre-trained transformers (GPT) are well-suited for natural language tasks like building SQL queries from text input. ," B, C, and D are not as suitable for this natural language to SQL task compared to GPT models. "
A company built a deep learning model for object detection and deployed the model to production. Which AI process occurs when the model analyzes a new image to identify objects?,Inference,Training; Model deployment; Bias correction," Inference occurs when a deployed model analyzes new data (in this case, a new image) to identify objects. "," A: Training happens before deployment. C: Model deployment is the process of making the model available for use. D: Bias correction is part of the development process, not the analysis of new data. "
An AI practitioner is building a model to generate images of humans in various professions. The AI practitioner discovered that the input data is biased and that specific attributes affect the image generation and create bias in the model. Which technique will solve the problem?,Data augmentation for imbalanced classes,Model monitoring for class distribution; Retrieval Augmented Generation (RAG); Watermark detection for images, Data augmentation for imbalanced classes can help solve bias issues in image generation by increasing the diversity of the training data. ," B: Model monitoring doesn't directly address input data bias. C: RAG is for enhancing language models with external knowledge, not for addressing image bias. D: Watermark detection is unrelated to addressing bias in image generation. "
A company is implementing the Amazon Titan foundation model (FM) by using Amazon Bedrock. The company needs to supplement the model by using relevant data from the company’s private data sources. Which solution will meet this requirement?,Create an Amazon Bedrock knowledge base,Use a different FM; Choose a lower temperature value; Enable model invocation logging, Creating an Amazon Bedrock knowledge base allows the company to supplement the Titan FM with relevant data from private sources. ," A: Changing the FM doesn't address the need to use private data. B: Temperature affects output randomness, not data integration. D: Invocation logging doesn't add private data to the model. "
A medical company is customizing a foundation model (FM) for diagnostic purposes. The company needs the model to be transparent and explainable to meet regulatory requirements. Which solution will meet these requirements?,"Generate simple metrics, reports, and examples by using Amazon SageMaker Clarify",Configure the security and compliance by using Amazon Inspector; Encrypt and secure training data by using Amazon Macie; Gather more data. Use Amazon Rekognition to add custom labels to the data," Amazon SageMaker Clarify can generate simple metrics, reports, and examples to make the model transparent and explainable, meeting regulatory requirements. "," A: Amazon Inspector is for security assessment, not model explainability. C: Amazon Macie is for data security, not model transparency. D: Adding more data and custom labels doesn't directly address explainability. "
A company wants to deploy a conversational chatbot to answer customer questions. The chatbot is based on a fine-tuned Amazon SageMaker JumpStart model. The application must comply with multiple regulatory frameworks. Which capabilities can the company show compliance for?,Threat detection; Data protection,Auto-scaling inference endpoints; Cost optimization; Loosely coupled microservices,"Data Protection
Amazon SageMaker provides robust data protection capabilities, which are crucial for regulatory compliance34. Some key features include:
Encryption for data at rest and in transit, using AWS Key Management Service (KMS) for key management3
Network isolation options, allowing training jobs and model deployments to run in isolated Amazon Virtual Private Cloud (VPC) environments3
Secure HTTPS endpoints for inference, protected by SSL/TLS encryption3
These data protection measures help ensure that sensitive customer data used in the chatbot is securely handled, aligning with various compliance requirements.
Threat Detection
SageMaker offers several features that support threat detection, which is essential for maintaining a secure and compliant environment35:
Integration with AWS CloudTrail and Amazon CloudWatch for logging and monitoring of SageMaker operations3
Ability to use Amazon GuardDuty to detect unusual activities and potential threats3
Continuous monitoring capabilities using Amazon CloudWatch to track operational metrics and logs3
Additionally, SageMaker can be integrated with other AWS security services like AWS Security Hub, which provides a comprehensive view of security state and helps check compliance against industry standards and best practices","While auto scaling inference endpoints (A), cost optimization (D), and loosely coupled microservices (E) are valuable features of Amazon SageMaker, they are not directly related to regulatory compliance in the context of the given scenario.
By focusing on data protection and threat detection capabilities, the company can demonstrate its commitment to maintaining a secure and compliant environment for its conversational chatbot application"
A company is training a foundation model (FM). The company wants to increase the accuracy of the model up to a specific acceptance level. Which solution will meet these requirements?,Increase the epochs,Decrease the batch size; Decrease the epochs; Increase the temperature parameter," Increasing the epochs allows for more training iterations, potentially increasing the model's accuracy up to a specific acceptance level. "," A: Decreasing batch size may affect training stability but doesn't directly increase accuracy. C: Decreasing epochs would reduce training time and potentially accuracy. D: Temperature affects output randomness during inference, not training accuracy. "
A company is building a large language model (LLM) question-answering chatbot. The company wants to decrease the number of actions call center employees need to take to respond to customer questions. Which business objective should the company use to evaluate the effect of the LLM chatbot?,Average call duration,Website engagement rate; Corporate social responsibility; Regulatory compliance, Average call duration is a relevant business objective to evaluate the effect of an LLM chatbot on call center efficiency. , A: Website engagement rate is not directly related to call center operations. C: Corporate social responsibility is not a specific metric for chatbot effectiveness. D: Regulatory compliance is important but not a direct measure of chatbot impact on call center operations. 
Which functionality does Amazon SageMaker Clarify provide?,Identifies potential bias during data preparation,Integrates a Retrieval Augmented Generation (RAG) workflow; Monitors the quality of ML models in production; Documents critical details about ML models," Amazon SageMaker Clarify identifies potential bias during data preparation, among other capabilities related to model explainability and fairness. "," A: RAG workflow integration is not a primary function of SageMaker Clarify. B: Model quality monitoring in production is done by SageMaker Model Monitor, not Clarify. C: Documenting model details is typically done with Model Cards, not Clarify. "
"A company is developing a new model to predict the prices of specific items. The model performed well on the training dataset. When the company deployed the model to production, the model’s performance decreased significantly. What should the company do to mitigate this problem?",Increase the volume of data that is used in training,Reduce the volume of data that is used in training; Add hyperparameters to the model; Increase the model training time, Increasing the volume of data used in training can help improve model performance in production by providing more diverse examples and reducing overfitting. , A: Reducing training data volume would likely worsen performance. B: Adding hyperparameters without addressing data issues may not solve the problem. D: Increasing training time alone doesn't necessarily improve generalization to production data. 
An eCommerce company wants to build a solution to determine customer sentiments based on written customer reviews of products. Which AWS services meet these requirements? (Choose two.),Amazon Comprehend; Amazon Bedrock,Amazon Lex; Amazon Polly; Amazon Rekognition, Amazon Comprehend (B) and Amazon Bedrock (D) are suitable AWS services for determining customer sentiments based on written reviews. ," A: Amazon Lex is for building conversational interfaces, not sentiment analysis. C: Amazon Polly is for text-to-speech, not sentiment analysis. E: Amazon Rekognition is for image and video analysis, not text sentiment analysis. "
A company wants to use large language models (LLMs) with Amazon Bedrock to develop a chat interface for the company’s product manuals. The manuals are stored as PDF files. Which solution meets these requirements MOST cost-effectively?,Upload PDF documents to an Amazon Bedrock knowledge base. Use the knowledge base to provide context when users submit prompts to Amazon Bedrock.,Use prompt engineering to add one PDF file as context to the user prompt when the prompt is submitted to Amazon Bedrock; Use prompt engineering to add all the PDF files as context to the user prompt when the prompt is submitted to Amazon Bedrock; Use all the PDF documents to fine-tune a model with Amazon Bedrock. Use the fine-tuned model to process user prompts., Using a knowledge base provides the most cost-effective way to incorporate context from PDF manuals into prompts. , A: Adding one PDF file is limited and inefficient. It doesn't provide comprehensive context and requires frequent updates. B: Adding all PDFs to each prompt is costly and impractical. It increases token usage and processing time significantly. C: Fine-tuning is more expensive and time-consuming than using a knowledge base. It requires specialized expertise and ongoing maintenance. 
A social media company wants to use a large language model (LLM) for content moderation. The company wants to evaluate the LLM outputs for bias and potential discrimination against specific groups or individuals. Which data source should the company use to evaluate the LLM outputs with the LEAST administrative effort?,Benchmark datasets,User-generated content; Moderation logs; Content moderation guidelines," Benchmark datasets provide standardized, pre-labeled data for evaluating LLM outputs with minimal administrative effort. "," A: User-generated content is inconsistent and requires labeling, which is time-consuming and may introduce bias. B: Moderation logs may be biased or incomplete, and don't provide a standardized evaluation metric. C: Guidelines don't provide actual data for evaluation, only theoretical standards. "
A company wants to use a pre-trained generative AI model to generate content for its marketing campaigns. The company needs to ensure that the generated content aligns with the company’s brand voice and messaging requirements. Which solution meets these requirements?,Create effective prompts that provide clear instructions and context to guide the model’s generation,"Optimize the model’s architecture and hyperparameters to improve the model’s overall performance; Increase the model’s complexity by adding more layers to the model’s architecture; Select a large, diverse dataset to pre-train a new generative model", Creating effective prompts with clear instructions and context is the most direct way to align generated content with brand requirements. ," A: Optimizing architecture doesn't target brand alignment specifically and may not improve content relevance. B: Increasing complexity doesn't necessarily improve brand alignment and may lead to overfitting. D: Pre-training a new model is unnecessary, costly, and time-consuming when prompt engineering can achieve the desired results. "
A loan company is building a generative AI-based solution to offer new applicants discounts based on specific business criteria. The company wants to build and use an AI model responsibly to minimize bias that could negatively affect some customers. Which actions should the company take to meet these requirements? (Choose two.),Detect imbalances or disparities in the data; Evaluate the model’s behavior so that the company can provide transparency to stakeholders,Ensure that the model runs frequently; Use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) technique to ensure that the model is 100% accurate; Ensure that the model’s inference time is within the accepted limits, Detecting data imbalances (A) and evaluating model behavior (C) are key steps in responsible AI development to minimize bias. ," B: Frequent runs don't address bias issues; they may reinforce existing biases. D: ROUGE is a text summarization metric, not for bias detection. E: Inference time isn't related to bias mitigation; it's a performance metric. "
A company is using an Amazon Bedrock base model to summarize documents for an internal use case. The company trained a custom model to improve the summarization quality. Which action must the company take to use the custom model through Amazon Bedrock?,Grant access to the custom model in Amazon Bedrock,Purchase Provisioned Throughput for the custom model; Deploy the custom model in an Amazon SageMaker endpoint for real-time inference; Register the model with the Amazon SageMaker Model Registry,"This is the correct step that the company needs to take before using their customized model in Amazon Bedrock. After creating and training a custom model, it's necessary to purchase Provisioned Throughput to make the model available for inference15.
Purchasing Provisioned Throughput is a crucial step in the process of utilizing a custom model in Amazon Bedrock. This action ensures that the necessary computational resources are allocated to support the model's performance and availability. By purchasing Provisioned Throughput, the company can access their custom model on-demand and use it for tasks such as document summarization5.
It's important to note that while custom model creation and training are essential steps, they are not sufficient on their own to make the model usable through Amazon Bedrock. The purchase of Provisioned Throughput is the final step that enables the company to leverage their custom model alongside other foundation models in the Bedrock environment","B. Deploying the custom model in an Amazon SageMaker endpoint is not necessary when using Amazon Bedrock, as Bedrock manages the infrastructure and deployment aspects.
C. Registering the model with the Amazon SageMaker Model Registry is not a requirement for using custom models in Amazon Bedrock.
D. While access control is important, simply granting access to the custom model in Amazon Bedrock is not sufficient to make it operational. The crucial step is purchasing Provisioned Throughput.
By purchasing Provisioned Throughput, the company ensures that their custom model for improved document summarization is fully integrated into the Amazon Bedrock ecosystem and ready for use in their internal applications."
A company needs to choose a model from Amazon Bedrock to use internally. The company must identify a model that generates responses in a style that the company’s employees prefer. What should the company do to meet these requirements?,Evaluate the models by using a human workforce and custom prompt datasets,Evaluate the models by using built-in prompt datasets; Use public model leaderboards to identify the model; Use the model InvocationLatency runtime metrics in Amazon CloudWatch when trying models, Evaluating models using a human workforce and custom prompt datasets provides the most accurate assessment of employee preferences for response style. , A: Built-in datasets may not reflect company-specific preferences. C: Public leaderboards don't account for company-specific needs. D: InvocationLatency metrics don't measure response style preferences. 
A student at a university is copying content from generative AI to write essays. Which challenge of responsible generative AI does this scenario represent?,Plagiarism,Toxicity; Hallucinations; Privacy, Copying content from generative AI for essays is a clear example of plagiarism in academic settings. ," A: Toxicity relates to harmful content, not copying. B: Hallucinations refer to false information, not copying. D: Privacy concerns data protection, not academic integrity. "
A company needs to build its own large language model (LLM) based on only the company’s private data. The company is concerned about the environmental effect of the training process. Which Amazon EC2 instance type has the LEAST environmental effect when training LLMs?,Amazon EC2 Trn series,Amazon EC2 C series; Amazon EC2 G series; Amazon EC2 P series," Amazon EC2 Trn series is specifically designed for efficient ML training, including LLMs, with the least environmental impact. "," A: C series is for compute-optimized workloads, not ML-specific. B: G series is for graphics-intensive tasks. C: P series, while used for ML, is not as environmentally optimized as Trn series. "
A company wants to build an interactive application for children that generates new stories based on classic stories. The company wants to use Amazon Bedrock and needs to ensure that the results and topics are appropriate for children. Which AWS service or feature will meet these requirements?,Guardrails for Amazon Bedrock,Amazon Rekognition; Amazon Bedrock playgrounds; Agents for Amazon Bedrock, Guardrails for Amazon Bedrock allow content filtering and ensure appropriate responses for specific audiences like children. ," A: Rekognition is for image and video analysis. B: Playgrounds are for testing, not content control. D: Agents are for task automation, not content filtering. "
A company is building an application that needs to generate synthetic data that is based on existing data. Which type of model can the company use to meet this requirement?,Generative adversarial network (GAN),XGBoost; Residual neural network; WaveNet, Generative adversarial networks (GANs) are specifically designed to generate synthetic data based on existing data. ," B: XGBoost is for prediction, not data generation. C: Residual neural networks are for deep learning, not specifically for data generation. D: WaveNet is for audio generation, not general synthetic data. "
A digital devices company wants to predict customer demand for memory hardware. The company does not have coding experience or knowledge of ML algorithms and needs to develop a data-driven predictive model. The company needs to perform analysis on internal data and external data. Which solution will meet these requirements?,Import the data into Amazon SageMaker Canvas. Build ML models and demand forecast predictions by selecting the values in the data from SageMaker Canvas,Store the data in Amazon S3. Create ML models and demand forecast predictions by using Amazon SageMaker built-in algorithms that use the data from Amazon S3; Import the data into Amazon SageMaker Data Wrangler. Create ML models and demand forecast predictions by using SageMaker built-in algorithms; Import the data into Amazon SageMaker Data Wrangler. Build ML models and demand forecast predictions by using an Amazon Personalize Trending-Now recipe," Amazon SageMaker Canvas allows non-coders to build ML models and make predictions using a visual interface, suitable for the company's lack of coding experience. "," A, B: Require coding skills. C: Amazon Personalize is for personalization, not general demand forecasting. "
A company has installed a security camera. The company uses an ML model to evaluate the security camera footage for potential thefts. The company has discovered that the model disproportionately flags people who are members of a specific ethnic group. Which type of bias is affecting the model output?,Sampling bias,Measurement bias; Observer bias; Confirmation bias," Sampling bias occurs when the training data doesn't represent the population fairly, leading to disproportionate flagging of specific groups. ", A: Measurement bias relates to data collection errors. C: Observer bias is about the researcher's influence. D: Confirmation bias is about favoring pre-existing beliefs. 
A company is building a customer service chatbot. The company wants the chatbot to improve its responses by learning from past interactions and online resources. Which AI learning strategy provides this self-improvement capability?,Reinforcement learning with rewards for positive customer feedback,Supervised learning with a manually curated dataset of good responses and bad responses; Unsupervised learning to find clusters of similar customer inquiries; Supervised learning with a continuously updated FAQ database, Reinforcement learning with rewards for positive customer feedback allows the chatbot to improve based on past interactions and adapt to new information. ," A, D: Don't allow for continuous improvement. C: Doesn't incorporate feedback for improvement. "
An AI practitioner has built a deep learning model to classify the types of materials in images. The AI practitioner now wants to measure the model performance. Which metric will help the AI practitioner evaluate the performance of the model?,Confusion matrix,Correlation matrix; R2 score; Mean squared error (MSE)," A confusion matrix provides a detailed breakdown of correct and incorrect classifications, essential for evaluating image classification model performance. "," B: For correlation analysis, not classification. C: For regression models. D: For regression models, not classification. "
A company has built a chatbot that can respond to natural language questions with images. The company wants to ensure that the chatbot does not return inappropriate or unwanted images. Which solution will meet these requirements?,Implement moderation APIs,Retrain the model with a general public dataset; Perform model validation; Automate user feedback integration, Implementing moderation APIs allows for real-time filtering of inappropriate or unwanted images before they are returned by the chatbot. ," B: Retraining doesn't guarantee appropriate images. C: Model validation alone doesn't prevent inappropriate images. D: User feedback is reactive, not proactive. "
An AI practitioner is using an Amazon Bedrock base model to summarize session chats from the customer service department. The AI practitioner wants to store invocation logs to monitor model input and output data. Which strategy should the AI practitioner use?,Enable invocation logging in Amazon Bedrock,Configure AWS CloudTrail as the logs destination for the model; Configure AWS Audit Manager as the logs destination for the model; Configure model invocation logging in Amazon EventBridge, Enabling invocation logging in Amazon Bedrock is the most direct and appropriate method to store model input and output data for monitoring purposes. ," A: CloudTrail is for API activity, not model I/O. C: Audit Manager is for compliance, not logging. D: EventBridge is for event-driven architectures, not logging. "
A company is building an ML model to analyze archived data. The company must perform inference on large datasets that are multiple GBs in size. The company does not need to access the model predictions immediately. Which Amazon SageMaker inference option will meet these requirements?,Batch transform,Real-time inference; Serverless inference; Asynchronous inference, Batch transform in Amazon SageMaker is designed for large-scale inference on datasets where immediate results are not required. ," B: Real-time inference is for immediate results. C: Serverless inference is for variable workloads. D: Asynchronous inference is for long-running predictions, but not as suitable for very large datasets. "
Which term describes the numerical representations of real-world objects and concepts that AI and natural language processing (NLP) models use to improve understanding of textual information?,Embeddings,Tokens; Models; Binaries, Embeddings are numerical representations of real-world objects and concepts used by AI and NLP models to understand textual information. ," B: Tokens are units of text, not numerical representations. C: Models process embeddings, they are not the representations themselves. D: Binaries are compiled code, not representations of concepts. "
"A research company implemented a chatbot by using a foundation model (FM) from Amazon Bedrock. The chatbot searches for answers to questions from a large database of research papers. After multiple prompt engineering attempts, the company notices that the FM is performing poorly because of the complex scientific terms in the research papers. How can the company improve the performance of the chatbot?",Use domain adaptation fine-tuning to adapt the FM to complex scientific terms,Use few-shot prompting to define how the FM can answer the questions; Change the FM inference parameters; Clean the research paper data to remove complex scientific terms, Domain adaptation fine-tuning allows the foundation model to adapt to the specific vocabulary and context of complex scientific terms. , A: Few-shot prompting may not be sufficient for complex scientific terms. C: Changing inference parameters won't address domain-specific knowledge gaps. D: Removing complex terms defeats the purpose of scientific content. 
A company wants to use a large language model (LLM) on Amazon Bedrock for sentiment analysis. The company needs the LLM to produce more consistent responses to the same input prompt. Which adjustment to an inference parameter should the company make to meet these requirements?,Decrease the temperature value,Increase the temperature value; Decrease the length of output tokens; Increase the maximum generation length, Decreasing the temperature value makes the model's outputs more deterministic and consistent. ," B: Increasing temperature leads to more randomness. C, D: Changing output length or maximum generation length doesn't directly affect consistency. "
A company wants to develop a large language model (LLM) application by using Amazon Bedrock and customer data that is uploaded to Amazon S3. The company’s security policy states that each team can access data for only the team’s own customers. Which solution will meet these requirements?,Create an Amazon Bedrock custom service role for each team that has access to only the team’s customer data,Create a custom service role that has Amazon S3 access. Ask teams to specify the customer name on each Amazon Bedrock request; Redact personal data in Amazon S3. Update the S3 bucket policy to allow team access to customer data; Create one Amazon Bedrock role that has full Amazon S3 access. Create IAM roles for each team that have access to only each team’s customer folders, Creating custom service roles for each team in Amazon Bedrock with access only to their customer data ensures proper data isolation and security. ," B, C, D: These options don't provide the required level of data isolation between teams or are more complex to implement and maintain. "
"A medical company deployed a disease detection model on Amazon Bedrock. To comply with privacy policies, the company wants to prevent the model from including personal patient information in its responses. The company also wants to receive notification when policy violations occur. Which solution meets these requirements?",Use Guardrails for Amazon Bedrock to filter content. Set up Amazon CloudWatch alarms for notification of policy violations,Use Amazon Macie to scan the model’s output for sensitive data and set up alerts for potential violations; Configure AWS CloudTrail to monitor the model’s responses and create alerts for any detected personal information; Implement Amazon SageMaker Model Monitor to detect data drift and receive alerts when model quality degrades," Guardrails for Amazon Bedrock allow content filtering, and CloudWatch alarms can be set up for policy violation notifications. "," A: Macie is for S3 data, not model outputs. B: CloudTrail is for API activity, not content filtering. D: Model Monitor is for model drift, not content filtering. "
"A company manually reviews all submitted resumes in PDF format. As the company grows, the company expects the volume of resumes to exceed the company’s review capacity. The company needs an automated system to convert the PDF resumes into plain text format for additional processing. Which AWS service meets this requirement?",Amazon Textract,Amazon Personalize; Amazon Lex; Amazon Transcribe," Amazon Textract is specifically designed to extract text from documents, including PDFs. ", B: Personalize is for recommendations. C: Lex is for conversational interfaces. D: Transcribe is for speech-to-text conversion. 
An education provider is building a question-and-answer application that uses a generative AI model to explain complex concepts. The education provider wants to automatically change the style of the model response depending on who is asking the question. The education provider will give the model the age range of the user who has asked the question. Which solution meets these requirements with the LEAST implementation effort?,Add a role description to the prompt context that instructs the model of the age range that the response should target,Fine-tune the model by using additional training data that is representative of the various age ranges that the application will support; Use chain-of-thought reasoning to deduce the correct style and complexity for a response suitable for that user; Summarize the response text depending on the age of the user so that younger users receive shorter responses, Adding a role description to the prompt context is the least effort solution to adapt the model's response style based on the user's age range. , A: Fine-tuning requires more effort. C: Chain-of-thought reasoning is more complex. D: Summarizing doesn't necessarily adapt the style appropriately. 
Which strategy evaluates the accuracy of a foundation model (FM) that is used in image classification tasks?,Measure the model’s accuracy against a predefined benchmark dataset,Calculate the total cost of resources used by the model; Count the number of layers in the neural network; Assess the color accuracy of images processed by the model, Measuring the model's accuracy against a predefined benchmark dataset is the most appropriate method to evaluate an image classification model's accuracy. , A: Cost calculation doesn't measure accuracy. C: Layer count doesn't indicate accuracy. D: Color accuracy alone doesn't evaluate overall classification performance. 
An accounting firm wants to implement a large language model (LLM) to automate document processing. The firm must proceed responsibly to avoid potential harms. What should the firm do when developing and deploying the LLM? (Choose two.),Include fairness metrics for model evaluation; Modify the training data to mitigate bias,Adjust the temperature parameter of the model; Avoid overfitting on the training data; Apply prompt engineering techniques, Including fairness metrics (A) and modifying training data to mitigate bias (C) are crucial steps in responsible AI development. , B: Adjusting temperature doesn't address bias. D: Avoiding overfitting is important but not specific to responsible AI. E: Prompt engineering doesn't directly address potential harms. 
"A company is building an ML model. The company collected new data and analyzed the data by creating a correlation matrix, calculating statistics, and visualizing the data. Which stage of the ML pipeline is the company currently in?",Exploratory data analysis,Data pre-processing; Feature engineering; Hyperparameter tuning," Creating correlation matrices, calculating statistics, and visualizing data are key activities in exploratory data analysis. ", A: Data pre-processing involves cleaning and transforming data. B: Feature engineering creates new features. D: Hyperparameter tuning optimizes model parameters. 
A company has documents that are missing some words because of a database error. The company wants to build an ML model that can suggest potential words to fill in the missing text. Which type of model meets this requirement?,BERT-based models,Topic modeling; Clustering models; Prescriptive ML models," BERT-based models are specifically designed for natural language processing tasks, including filling in missing words in text. "," A: Topic modeling identifies themes in text. B: Clustering groups similar data points. C: Prescriptive models suggest actions, not fill in text. "
A company wants to display the total sales for its top-selling products across various retail locations in the past 12 months. Which AWS solution should the company use to automate the generation of graphs?,Amazon Q in Amazon QuickSight,Amazon Q in Amazon EC2; Amazon Q Developer; Amazon Q in AWS Chatbot," Amazon Q in Amazon QuickSight is designed for generating visualizations and graphs from data, including sales data. "," A, B, D: These options are not specifically designed for automated graph generation in business intelligence contexts. "
A company is building a chatbot to improve user experience. The company is using a large language model (LLM) from Amazon Bedrock for intent detection. The company wants to use few-shot learning to improve intent detection accuracy. Which additional data does the company need to meet these requirements?,Pairs of user messages and correct user intents,Pairs of chatbot responses and correct user intents; Pairs of user messages and correct chatbot responses; Pairs of user intents and correct chatbot responses, Few-shot learning for intent detection requires examples of user messages paired with their correct intents. ," A, B, D: These pairs don't provide the correct input-output relationship for intent detection. "
A company is using few-shot prompting on a base model that is hosted on Amazon Bedrock. The model currently uses 10 examples in the prompt. The model is invoked once daily and is performing well. The company wants to lower the monthly cost. Which solution will meet these requirements?,Decrease the number of tokens in the prompt,Customize the model by using fine-tuning; Increase the number of tokens in the prompt; Use Provisioned Throughput," Decreasing the number of tokens in the prompt reduces the amount of data processed, lowering costs. ", A: Fine-tuning is more expensive. C: Increasing tokens raises costs. D: Provisioned Throughput is for high-volume use cases. 
An AI practitioner is using a large language model (LLM) to create content for marketing campaigns. The generated content sounds plausible and factual but is incorrect. Which problem is the LLM having?,Hallucination,Data leakage; Overfitting; Underfitting, Hallucination occurs when an LLM generates plausible but incorrect information. , A: Data leakage is about training data contamination. C: Overfitting causes poor generalization. D: Underfitting results in poor performance overall. 
An AI practitioner trained a custom model on Amazon Bedrock by using a training dataset that contains confidential data. The AI practitioner wants to ensure that the custom model does not generate inference responses based on confidential data. How should the AI practitioner prevent responses based on confidential data?,Remove the confidential data from the training dataset.,Delete the custom model. Retrain the custom model; Encrypt the confidential data in the inference responses by using Amazon SageMaker; Mask the confidential data in the inference responses by using dynamic data masking;Encrypt the confidential data in the custom model by using AWS Key Management Service (AWS KMS), Removing confidential data from the training set and retraining is the most effective way to prevent the model from using that data. ," B, C, D: These methods don't prevent the model from learning confidential data during training. "
A company has built a solution by using generative AI. The solution uses large language models (LLMs) to translate training manuals from English into other languages. The company wants to evaluate the accuracy of the solution by examining the text generated for the manuals. Which model evaluation strategy meets these requirements?,Bilingual Evaluation Understudy (BLEU),Root mean squared error (RMSE); Recall-Oriented Understudy for Gisting Evaluation (ROUGE); F1 score, BLEU is specifically designed to evaluate machine translation quality. , B: RMSE is for regression tasks. C: ROUGE is for summarization tasks. D: F1 score is a general classification metric. 
A large retailer receives thousands of customer support inquiries about products every day. The customer support inquiries need to be processed and responded to quickly. The company wants to implement Agents for Amazon Bedrock. What are the key benefits of using Amazon Bedrock agents that could help this retailer?,Automation of repetitive tasks and orchestration of complex workflows,Generation of custom foundation models (FMs) to predict customer needs; Automatically calling multiple foundation models (FMs) and consolidating the results; Selecting the foundation model (FM) based on predefined criteria and metrics," Amazon Bedrock agents are designed to automate repetitive tasks and orchestrate complex workflows, which is ideal for processing customer inquiries. "," A, C, D: These options don't accurately describe the key benefits of Amazon Bedrock agents for this use case. "
Which option is a benefit of ongoing pre-training when fine-tuning a foundation model (FM)?,Improves model performance over time,Helps decrease the model’s complexity; Decreases the training time requirement; Optimizes model inference time, Ongoing pre-training improves model performance over time by continually updating the model with new data and knowledge. , A: Pre-training doesn't decrease complexity. C: It typically increases training time. D: It doesn't directly optimize inference time. 
What are tokens in the context of generative AI models?,"Tokens are the basic units of input and output that a generative AI model operates on, representing words, subwords, or other linguistic units",Tokens are the mathematical representations of words or concepts used in generative AI models; Tokens are the pre-trained weights of a generative AI model that are fine-tuned for specific tasks; Tokens are the specific prompts or instructions given to a generative AI model to generate output," Tokens are the basic units of input and output for generative AI models, representing words, subwords, or other linguistic units. "," B: Embeddings, not tokens, are mathematical representations. C: Tokens aren't pre-trained weights. D: Tokens aren't prompts or instructions. "
A company wants to assess the costs that are associated with using a large language model (LLM) to generate inferences. The company wants to use Amazon Bedrock to build generative AI applications. Which factor will drive the inference costs?,Number of tokens consumed,Temperature value; Amount of data used to train the LLM; Total training time, The number of tokens consumed is the primary factor driving inference costs in Amazon Bedrock. ," B: Temperature doesn't affect cost. C, D: Training data amount and time don't directly impact inference costs. "
A company is using Amazon SageMaker Studio notebooks to build and train ML models. The company stores the data in an Amazon S3 bucket. The company needs to manage the flow of data from Amazon S3 to SageMaker Studio notebooks. Which solution will meet this requirement?,Configure SageMaker to use a VPC with an S3 endpoint,Use Amazon Inspector to monitor SageMaker Studio; Use Amazon Macie to monitor SageMaker Studio; Configure SageMaker to use S3 Glacier Deep Archive," Configuring SageMaker to use a VPC with an S3 endpoint allows secure, efficient data flow between S3 and SageMaker Studio notebooks. "," A, B: Inspector and Macie are for security monitoring, not data flow management. D: S3 Glacier Deep Archive is for long-term storage, not active use. "
A company has a foundation model (FM) that was customized by using Amazon Bedrock to answer customer queries about products. The company wants to validate the model’s responses to new types of queries. The company needs to upload a new dataset that Amazon Bedrock can use for validation. Which AWS service meets these requirements?,Amazon S3,Amazon Elastic Block Store (Amazon EBS); Amazon Elastic File System (Amazon EFS); AWS Snowcone, Amazon S3 is the most suitable service for uploading and storing datasets that Amazon Bedrock can use for model validation. ," B: EBS is for block storage attached to EC2 instances. C: EFS is for file storage, less suitable for this use case. D: Snowcone is for edge computing and data transfer, not ideal for this scenario. "
